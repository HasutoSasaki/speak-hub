# Hugging Face Agents Course 学習まとめ

## 学習範囲

- Unit 1: Introduction to Agents（全セクション）
- Unit 2: Introduction to Agentic Frameworks（イントロダクション）

---

## 1. LLMの本質

### LLMとは何か

LLM（Large Language Model）は、**入力されたトークン列の次のトークンを予測する**モデルである。
訓練データはテキストであり、出力もトークン（＝テキスト）である。

LLMの内部にHTTPリクエストを送る機能や計算を実行する仕組みは存在しない。
「234 × 567 は？」と聞かれたとき、LLMは実際に掛け算をしているのではなく、
訓練データ中の似たパターンから「それっぽい数字」を予測しているだけである。
そのため、大きな数の計算では間違えることがある。

### LLMにできること・できないこと

| できること                   | できないこと               |
| ---------------------------- | -------------------------- |
| テキストを入力として受け取る | 記憶を保持する             |
| テキストを出力する           | APIを叩く                  |
| パターンに基づく推論         | 計算を正確に実行する       |
| 文脈から次の単語を予測する   | リアルタイム情報を取得する |

> **重要**: LLMは純粋に「テキストイン・テキストアウト」のモデルである。
> これがエージェントの仕組みを理解する上での最も重要な前提となる。

---

## 2. メッセージと特殊トークン

### UIとモデルの入力のギャップ

チャットUIでは、ユーザーとモデルが一問一答で対話しているように見える。
しかし実態は全く異なる。

**LLMには記憶がない。** 3往復目の質問に対してモデルが過去の会話を「覚えている」ように見えるのは、
**過去のすべての会話が連結されて、毎回1本の長いテキストとして丸ごとモデルに渡されている**からである。

モデルは毎回この全体を「初めて読む文章」として処理して、その続きを生成しているだけである。

```
[1回目のユーザーの質問]
[1回目のモデルの回答]
[2回目のユーザーの質問]
[2回目のモデルの回答]
[3回目のユーザーの質問]　← 今ここ。モデルは上記すべてを毎回読み直す
```

### 特殊トークン（Special Tokens）

過去のやり取りを全部連結するだけでは、どこがユーザーの発言でどこがモデルの回答かわからなくなる。
この境界を示すために使われるのが**特殊トークン**である。

SmolLM2の場合：

```
<|im_start|>user
注文について相談したいです<|im_end|>
<|im_start|>assistant
かしこまりました。注文番号を教えていただけますか？<|im_end|>
<|im_start|>user
ORDER-123です<|im_end|>
<|im_start|>assistant
```

Llama 3.2の場合：

```
<|start_header_id|>user<|end_header_id|>
注文について相談したいです<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
...
```

### モデルごとにフォーマットが違う

特殊トークンの形式は**モデルごとに異なる**。
間違ったフォーマットで渡すと、モデルは特殊トークンを「意味のある区切り」として認識できず、
ただの文字列の一部として扱ってしまう。結果としてまともな応答ができなくなる。

### チャットテンプレート

チャットテンプレートは、Pythonのメッセージリスト（ChatML形式）を、
各モデルが期待する特殊トークン付きのプロンプト文字列に変換する仕組みである。

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-1.7B-Instruct")
rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```

`apply_chat_template` は、メッセージリストを受け取り、
モデルに合った特殊トークン付きのプロンプト文字列を返す関数である。

---

## 3. システムメッセージ（System Message）

### 役割

システムメッセージは、会話の一番最初に置かれ、**モデル全体の振る舞い方を指示する**永続的な指示である。
ユーザーには見えないが、毎回プロンプトの先頭に含まれる。

```
<|im_start|>system
あなたは丁寧なカスタマーサポートです。<|im_end|>
<|im_start|>user
注文について相談したいです<|im_end|>
<|im_start|>assistant
```

### システムメッセージに含まれる情報

| 情報               | 説明                                                     |
| ------------------ | -------------------------------------------------------- |
| モデルの人格・制約 | 「丁寧に応答してください」「大阪弁で話してください」など |
| 使えるツールの一覧 | ツール名、説明、引数の型、出力の型                       |
| 思考プロセスの手順 | Thought → Action → Observationサイクルの指示             |
| 出力形式の指定     | 「JSON形式で出力して」「Pythonコードで出力して」など     |

---

## 4. ツール（Tools）

### ツールとは

ツールとは、**LLMに与えられる関数**のことである。
LLMは訓練データに基づく静的な知識しか持たないため、リアルタイム情報の取得や正確な計算などは
ツールを通じて行う必要がある。

### LLMはツールを「使えない」

LLMはテキストしか扱えないため、**自分自身でAPIを叩くことはできない。**

LLMがツールを「使う」とは、実際には以下の流れである：

1. ユーザーが「パリの天気は？」と聞く
2. LLMが、システムメッセージに記載されたツール一覧を参照し、「weatherツールを使うべきだ」と判断する
3. LLMが `call weather_tool('Paris')` のような**テキスト**を出力する（ツール呼び出しの指示文を生成するだけ）
4. **エージェント（LLMの外側のプログラム）** がその出力テキストを解析して、実際にAPIを実行する
5. APIの結果をメッセージとして会話に追加する
6. LLMが結果を含む会話全体を読んで、ユーザーへの自然な返答を生成する

> **重要**: LLMは「このツールを使ってくれ」というテキストを書いているだけで、
> 実際にAPIを叩いているのはLLMの外側にいるエージェントプログラムである。

### ツールに必要な4つの情報

LLMにツールの存在を教えるには、以下の4つの情報が必要である：

1. **ツール名**（例: `calculator`）
2. **何をするかの説明**（例: 「2つの整数を掛ける」）
3. **引数と型**（例: `a: int, b: int`）
4. **出力の型**（例: `int`）

これらの情報はシステムメッセージの中に記述される。

### `@tool` デコレータによる自動抽出

Pythonの関数定義には、上記4つの情報がすべて含まれている：

```python
@tool
def get_current_time_in_timezone(timezone: str) -> str:
#   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^  ← ① ツール名（関数名）
#                                  ^^^^^^^^^^^^^ ← ③ 引数と型
#                                                    ^^^ ← ④ 出力の型
    """A tool that fetches the current local time in a specified timezone.
    # ↑ ② 何をするかの説明（docstring）
    Args:
        timezone: A string representing a valid timezone
    """
    ...
```

`@tool` デコレータは、Pythonの `inspect` モジュールを使って
関数名、docstring、引数の型ヒント、戻り値の型ヒントを自動抽出し、
LLM向けの説明文を生成する。

```
Tool Name: get_current_time_in_timezone,
Description: A tool that fetches the current local time in a specified timezone.,
Arguments: timezone: str,
Outputs: str
```

だからこそ、ツール定義では**型ヒントとdocstringを正確に書くことが重要**である。

### ツール情報のコンテキストウィンドウ圧迫問題

ツールの説明文は毎回システムメッセージに含まれるため、ツールが増えるほどコンテキストウィンドウを消費する。
そのため、ツールの説明は**正確かつ簡潔**に書くべきである。

---

## 5. エージェント（Agent）

### エージェントとは

エージェントとは、**LLM（思考するが行動できない）とツール（行動できるが思考できない）の橋渡しをするプログラム**である。

LLMが出力した「このツールを使ってほしい」というテキストを解析し、
実際にツールを実行し、その結果をまたLLMに返す。

### エージェントの内部構造

エージェントの中身は驚くほどシンプルで、**whileループとif文**で構成される：

```python
while True:
    # 1. 会話全体をLLMに渡してテキストを生成させる
    response = llm.generate(messages)  # ← Stop（生成を止める）

    # 2. 出力テキストをパースする              ← Parse
    if has_tool_call(response):
        # 3. テキストからツール名と引数を抽出
        tool_name, args = parse_tool_call(response)

        # 4. 対応するツール（ただの関数）を実行
        result = tools[tool_name](**args)

        # 5. 結果を会話に追加してループの先頭に戻る
        messages.append({"role": "tool", "content": result})
    else:
        # ツール呼び出しがなければ最終回答として返す
        return response
```

エージェントが柔軟に見える理由は、**賢い判断をしているのはすべてLLM側**だからである。
エージェント自体は「LLMの出力にツール呼び出しがあれば実行する、なければ終了する」
という単純なルールで動いているだけである。

### Stop and Parse（停止とパース）

LLMは次のトークンを予測し続けるモデルであるため、
JSONやコードを出力した後も止まらずにテキストを生成し続ける可能性がある。

そのため：

1. **Stop**: LLMがアクションのテキストを出力し終わったら生成を止める
2. **Parse**: エージェントがそのテキストを解析して、ツール呼び出しかどうか判定する

**まずLLMを止めないと、パースすべきテキストが確定しない。**

### 無限ループの防止

ツールがエラーを返し続けた場合、エージェントは永遠にループしてしまう可能性がある。
そのため、実際のエージェントでは**ループの最大回数（max_steps）** を設定する。

---

## 6. Thought-Action-Observation サイクル

### 概要

エージェントの動作は、3つのステップの繰り返しで構成される：

1. **Thought（思考）**: LLMが「何をすべきか」を考える
2. **Action（行動）**: LLMが出力したツール呼び出しをエージェントが実行する
3. **Observation（観察）**: ツールの実行結果が会話に追加され、LLMがそれを読む

この3つが**whileループの中で、目的が達成されるまで繰り返される**。

### 具体例：天気エージェント

ユーザー: 「ニューヨークの天気は？」

1. **Thought**: 「ユーザーはNYの天気を知りたい。weather APIツールを使おう」
2. **Action**: `{"action": "get_weather", "action_input": {"location": "New York"}}` を出力
3. **Observation**: APIから「曇り、15℃、湿度60%」が返ってくる → 会話に追加
4. **再びThought**: 「天気データが手に入った。ユーザーに回答をまとめよう」
5. **最終Action**: ユーザーへの自然言語での回答を生成 → ループ終了

### Observationの重要性

LLMには記憶がないため、ツールの結果は**メッセージとして会話に追加**する必要がある。
追加しないと、次のThoughtでLLMはツールの結果を知ることができない。

### システムメッセージとの関係

LLMが「まず考えて、次にツールを使って、結果を観察する」という手順で動けるのは、
システムメッセージの中に**このサイクルの手順自体が指示として書かれている**からである。
LLMに組み込まれた能力ではなく、**プロンプトで指示されたパターン**である。

---

## 7. 思考テクニック：CoT と ReAct

### Chain-of-Thought（CoT）

**外部ツールを使わず**に、ステップバイステップで考える手法。
プロンプトに「ステップバイステップで考えましょう」と書くだけで、LLMの推論精度が上がる。

```
Question: 200の15%はいくつ？
Thought: ステップバイステップで考えよう。10%は20、5%は10、だから15%は30。
Answer: 30
```

数学や論理の問題に向いている。

### ReAct（Reasoning + Acting）

**思考とアクションを交互に行う**手法。外部ツールを使って情報を取得しながら推論する。

```
Thought: パリの最新の天気を調べる必要がある。
Action: Search["weather in Paris"]
Observation: 18°C、曇り。
Thought: 天気がわかったので回答をまとめよう。
Action: Finish["パリは18°C、曇りです。"]
```

情報取得や複数ステップのタスクに向いている。

### 比較

| 特徴                       | CoT                      | ReAct                                |
| -------------------------- | ------------------------ | ------------------------------------ |
| ステップバイステップの論理 | あり                     | あり                                 |
| 外部ツールの使用           | なし                     | あり                                 |
| 適したタスク               | 論理、数学、内部的な推論 | 情報取得、動的な複数ステップのタスク |

### 訓練レベルの思考

CoT・ReActは**プロンプティング戦略**（どのモデルでも使える）であるのに対し、
DeepSeek R1やOpenAI o1のようなモデルは、`<think>` `</think>` のような特殊トークンを使って
**訓練レベルで「まず考えてから答える」ことを学習**している。

---

## 8. アクションの形式：JSON Agent と Code Agent

### JSON Agent

ツール呼び出しを**JSON形式**で表現する：

```json
{
  "action": "get_weather",
  "action_input": { "location": "New York" }
}
```

### Code Agent

ツール呼び出しを**Pythonコード**で表現する：

```python
result = get_weather("New York")
print(result)
```

### 比較

| 特徴           | JSON Agent                     | Code Agent                     |
| -------------- | ------------------------------ | ------------------------------ |
| 安全性         | 高い（構造化されたデータのみ） | 低い（悪意あるコードのリスク） |
| 表現力         | 限定的                         | 高い（ループ、条件分岐が可能） |
| パースの容易さ | JSON解析のみ                   | コード実行が必要               |

Code Agentを使う場合は、サンドボックス環境など**セキュリティ対策**が必要である。

---

## 9. smolagents チュートリアル

### smolagents とは

HuggingFace製の軽量エージェントフレームワーク。
**CodeAgent**（Pythonコードでアクションを表現するエージェント）を中心に設計されている。

### コード構造と概念の対応

```python
agent = CodeAgent(
    model=model,                    # ← LLMエンジン
    tools=[final_answer, ...],      # ← エージェントが使えるツールのリスト
    max_steps=6,                    # ← 無限ループ防止の上限
    prompt_templates=prompt_templates # ← システムメッセージのテンプレート
)
```

### ツールの追加

tools配列にツールを追加すると、`@tool`デコレータが各ツールの説明文を自動生成し、
**システムメッセージの中にそれらが追加される**。

```python
# ツールを追加するとエージェントの能力が変わる
tools=[final_answer, DuckDuckGoSearchTool(), image_generation_tool, get_current_time_in_timezone]
```

tools配列に入れ忘れたツールは、コード上に関数が存在していてもLLMはその存在を知らないため使えない。

---

## 10. エージェントフレームワークに必要な6つの機能

Unit2のイントロダクションで挙げられた、フレームワークが提供すべき機能：

| 機能                                 | 説明                                                                                   |
| ------------------------------------ | -------------------------------------------------------------------------------------- |
| LLMエンジン                          | システムを動かすLLM（GPT-4、Llama、Qwenなど）                                          |
| ツールのリスト                       | エージェントが使えるツールの配列                                                       |
| パーサー                             | LLMの出力テキストからツール呼び出し情報を抽出する仕組み                                |
| パーサーと同期したシステムプロンプト | システムプロンプトが指示する出力形式と、パーサーが解析する形式が一致している必要がある |
| メモリシステム                       | LLMに記憶がないため、過去の会話やツール結果をまとめて渡す仕組み                        |
| エラーログとリトライ                 | エラーの分析用ログと、失敗時にLLMが再試行する仕組み                                    |

### パーサーとシステムプロンプトの「同期」が必要な理由

システムプロンプトが「Pythonコードで出力して」と指示しているのに、
パーサーがJSON形式を探す仕様だった場合、LLMがPythonコードを出力しても
パーサーは「ツール呼び出しが見つからない」と判断してしまう。
この2つは**同じ形式を前提にしていないと動かない**。

---

## 全体像のまとめ

ユーザーがチャットで質問してから回答が返るまでの裏側の流れ：

1. ユーザーの質問が届く
2. 過去の会話すべて（system + user + assistant + tool）が**特殊トークンで区切られ、1本のテキストに連結**される
3. LLMがそのテキスト全体を読んで、**Thought（思考）** する
4. LLMが次のアクションをテキストとして出力する → **Stop**（生成を止める）
5. エージェントが出力を**Parse**（解析）する
6. ツール呼び出しが含まれていれば → **Action（行動）**: エージェントがツールを実行する
7. ツールの結果が**Observation（観察）** として会話に追加される
8. 3に戻る（whileループ）
9. ツール呼び出しが含まれていなければ → 最終回答としてユーザーに返す

この一連の流れを支えているのが：

- **特殊トークン**: 各メッセージの役割を区切る
- **チャットテンプレート**: メッセージリストを正しいフォーマットに変換する
- **システムメッセージ**: モデルの振る舞い、ツール情報、思考手順を定義する
- **エージェント**: シンプルなwhileループでLLMとツールの橋渡しをする
- **LLM**: 思考のすべてを担い、テキストとしてアクションの指示を出す
